{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9de04a",
   "metadata": {},
   "source": [
    "#### Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cbb63",
   "metadata": {},
   "source": [
    "#### Download NLTK resources (only needed first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ace82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77425ca0",
   "metadata": {},
   "source": [
    "#### Define custom text preprocessor & tokenizer\n",
    "We will: \n",
    "* Remove HTML-tag and any residual markup\n",
    "* Lowercase all text\n",
    "* Remove punctuation and digits\n",
    "* Tokenize on whitespace or with a regex tokenizer\n",
    "* Remove stop-word using NLTK standard English list\n",
    "* Porter Stemming to reduce terms to root forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer    = PorterStemmer()\n",
    "\n",
    "def custom_preprocessor(text: str) -> str:\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # Remove punctuation (keep only word chars and whitespace)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return text\n",
    "\n",
    "def custom_tokenizer(text: str) -> list[str]:\n",
    "    # Basic word tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Stop-word removal\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25912ef0",
   "metadata": {},
   "source": [
    "#### Load and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/X_train.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "X_test  = pd.read_csv('data/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd28dd19",
   "metadata": {},
   "source": [
    "#### Drop coffee_id column but keep aside if needed for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6678e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = X_train.pop('coffee_id')\n",
    "test_ids  = X_test.pop('coffee_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868db584",
   "metadata": {},
   "source": [
    "#### Build sub-pipelines for attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric pipeline for 100g-USD\n",
    "num_pipe = Pipeline([\n",
    "    # log(1+x) to reduce skew\n",
    "    ('log',   FunctionTransformer(np.log1p, validate=True)),\n",
    "    # standardize\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Categorical pipeline for roaster, roast, origin\n",
    "cat_pipe = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# Text pipeline for review\n",
    "txt_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        preprocessor=custom_preprocessor,\n",
    "        tokenizer=custom_tokenizer,\n",
    "        token_pattern=None,\n",
    "        stop_words=None,        # we handle stop‚Äêwords in tokenizer\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5,\n",
    "        max_df=0.95,\n",
    "        max_features=1000\n",
    "    )),\n",
    "    ('select', SelectKBest(chi2, k=500)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40630f07",
   "metadata": {},
   "source": [
    "#### Combine everything into a ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipe,   ['100g_USD']),\n",
    "    ('cat', cat_pipe,   ['roaster', 'roast', 'origin']),\n",
    "    ('txt', txt_pipe,   'review'),\n",
    "], remainder='drop', verbose_feature_names_out=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401fcfc",
   "metadata": {},
   "source": [
    "#### Fit the preprocessing on the training data, transform both train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train, y_train.values.ravel())\n",
    "X_test_transformed  = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c316ae4",
   "metadata": {},
   "source": [
    "#### Inspect outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9859f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed X_train shape:\", X_train_transformed.shape)\n",
    "print(\"Transformed X_test  shape:\", X_test_transformed.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
